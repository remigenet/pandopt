{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226e75b7-5893-4774-87dd-6b29ed9db6a5",
   "metadata": {},
   "source": [
    "# Testing the rolling apply method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f43d4b-9c2a-4711-bae4-87b9bf9e68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandopt as pdo\n",
    "except:\n",
    "    import sys, os\n",
    "    sys.path.append('/'.join(os.getcwd().split('/')[:-2]))\n",
    "    import pandopt as pdo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import functools\n",
    "import time\n",
    "import plotly.express as px\n",
    "from typing import Callable, Dict\n",
    "import polars as pl\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def agg_sum(z):\n",
    "    return np.sum(z)\n",
    "\n",
    "def agg_mean(z):\n",
    "    return np.mean(z)\n",
    "\n",
    "def agg_max(z):\n",
    "    return np.max(z)\n",
    "\n",
    "def agg_min(z):\n",
    "    return np.min(z)\n",
    "\n",
    "def agg_std(z):\n",
    "    return np.std(z)\n",
    "\n",
    "def agg_sum(z):\n",
    "    return np.sum(z)\n",
    "\n",
    "def agg_custom(z):\n",
    "    # NOT HANDLED YET\n",
    "    print(z.shape)\n",
    "    if z['A',-1] > z['B',0]:\n",
    "        return np.median(z['C']) - np.std(np.log(z))\n",
    "    return np.mean(z[\"C\"]) / 2 / np.std(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19cdfea-c9b2-4a99-bff4-aa9024a53451",
   "metadata": {},
   "source": [
    "## Basic test and work setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12aa3d31-2a1c-49f3-b6b4-6aa334a75506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(method_func, df, method_name, check_metric):\n",
    "    if method_func is False:\n",
    "        print(f'{method_name:<15} {'NA':<15} {'NA':<15} {'Discarded':<10}')\n",
    "        return \n",
    "    t1 = time.time()\n",
    "    result = method_func(df)\n",
    "    result_np = result.to_numpy() if hasattr(result, 'to_numpy') else result\n",
    "    t2 = time.time()\n",
    "    execution_time = t2 - t1\n",
    "\n",
    "    if isinstance(result, pd.Series):\n",
    "        result_shape_str = f\"{result.shape[0]} x 1\"  \n",
    "    else:\n",
    "        result_shape_str = \" x \".join(map(str, result.shape)) if hasattr(result, 'shape') else \"N/A\"\n",
    "    \n",
    "    metric = check_metric(result.to_numpy() if hasattr(result, 'to_numpy') else result)\n",
    "    try:\n",
    "        print(f'{method_name:<15} {result_shape_str:<15} {metric:<15.5f} {execution_time:<10.5f}')\n",
    "    except:\n",
    "        print(f'{method_name:<15} {result_shape_str:<15} {metric:<15} {execution_time:<10}')\n",
    "    return execution_time\n",
    "\n",
    "def format_number(num):\n",
    "    \"\"\"\n",
    "    Convert a numerical value into a human-readable format,\n",
    "    adding suffixes like K, M, B, etc.\n",
    "    \"\"\"\n",
    "    for unit in ['', 'K', 'M', 'B', 'T']:\n",
    "        if abs(num) < 1000:\n",
    "            return f\"{num:3.0f}{unit}\"\n",
    "        num /= 1000.0\n",
    "    return f\"{num:.1f}T\"\n",
    "\n",
    "\n",
    "def run_compare(methods: Dict[str, Callable], x_seconds: float = 1.5, start_ten_exponent: int = 1, check_metric: Callable = lambda x: (1 + np.abs(np.max(x) - np.min(x))) / np.median(x)):\n",
    "    execution_times = {key: {} for key in methods}\n",
    "    ten_exponent = start_ten_exponent\n",
    "    while ten_exponent < 10: # Over than 9 become more SSD/RAM  - lazy loading options tes\n",
    "        df_size = int(10**ten_exponent)\n",
    "        formatted_size = format_number(df_size)\n",
    "    \n",
    "        print(f'\\ntesting {format_number(df_size)} rows')\n",
    "        print(f'{\"method_name\":<15} {\"result shape\":<15} {\"compare metric\":<15} {\"execution_time\":<10}')\n",
    "    \n",
    "        pandas_df = pd.DataFrame(np.random.randn(df_size, 4), columns=['A', 'B', 'C', 'D']).astype(np.float32)\n",
    "    \n",
    "        iteration_discards = []\n",
    "        for method_name, method_func in methods.items():\n",
    "            #Last is to avoid removing pandaopt at first iter due to compilation time\n",
    "            if (e_time:=timeit(method_func, pandas_df, method_name, check_metric)) and e_time > x_seconds and ten_exponent > start_ten_exponent: \n",
    "                iteration_discards.append(method_name)\n",
    "            execution_times[method_name][df_size] = e_time\n",
    "                \n",
    "        for method_name in iteration_discards:\n",
    "            methods[method_name] = False\n",
    "        \n",
    "        # Break the loop if all methods are over the time limit\n",
    "        if not methods:\n",
    "            print(\"All methods exceeded the time limit. Ending tests.\")\n",
    "            break\n",
    "        ten_exponent += 1\n",
    "    \n",
    "    results = pd.DataFrame(execution_times)\n",
    "    results_long = results.reset_index().melt(id_vars=['index'], var_name='Method', value_name='Execution Time')\n",
    "    results_long.rename(columns={'index': 'Rows'}, inplace=True)\n",
    "    fig = px.line(results_long, x='Rows', y='Execution Time', color='Method', log_x=True, log_y=True, title='Execution Times by Method')\n",
    "    fig.show()\n",
    "    \n",
    "    display(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba112b-3543-4de4-aae8-218c85c3ba79",
   "metadata": {},
   "source": [
    "## Comparison with rolling sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e956aaa-46d8-4e5d-a778-9e5344c365a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing 100 rows\n",
      "method_name     result shape    compare metric  execution_time\n"
     ]
    }
   ],
   "source": [
    "# Define test methods\n",
    "def pandas_apply(df):\n",
    "    return df.rolling(25).apply(agg_sum)\n",
    "\n",
    "def pandas_sum(df):\n",
    "    return df.rolling(25).sum()\n",
    "\n",
    "def pandopt_apply(df):\n",
    "    return pdo.DataFrame(df).rolling(25).apply(agg_sum)\n",
    "\n",
    "# No Polars nor Numpy as there is no direct implementation for this (to my knowledge, havent look long)\n",
    "\n",
    "methods = {\n",
    "    'pandas_apply': pandas_apply,\n",
    "    'pandas_sum': pandas_sum,\n",
    "    'pandopt_apply': pandopt_apply, \n",
    "}\n",
    "\n",
    "run_compare(methods, x_seconds = 1.5, start_ten_exponent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26581e0-8d61-400c-9b24-51520c07ff88",
   "metadata": {},
   "source": [
    "## Rolling apply functionnality added:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b79859-f0d4-433e-a090-e1486273fa3a",
   "metadata": {},
   "source": [
    "## Wider tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b1a09-2ea6-4fdf-904e-e05157d51158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def measure_performance(df, func, window_size=3):\n",
    "    try:\n",
    "        if type(df)==pdo.DataFrame:\n",
    "            operation = functools.partial(df.rolling(window=window_size).apply, func, axis = 0,raw=True)\n",
    "        else:\n",
    "            operation = functools.partial(df.rolling(window=window_size).apply, func, raw=True)\n",
    "        start_time = timeit.default_timer()\n",
    "        result = operation()\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        return np.sum(result.dropna().values), elapsed_time, None\n",
    "    except Exception as e:\n",
    "        return None, None, str(e)\n",
    "\n",
    "def run_tests(data_size, agg_funcs):\n",
    "    results = {}\n",
    "    total_tests = (data_size - 1) * len(agg_funcs) \n",
    "    progress_bar = tqdm.tqdm(total=total_tests, desc=\"Running Tests\", ncols=100)\n",
    "    test_type = 'rolling'\n",
    "    for test_num in range(1, data_size):\n",
    "        df_size = int(10**test_num)\n",
    "        for func in agg_funcs:\n",
    "            for test_iter in range(15):\n",
    "                pandas_df = pd.DataFrame(np.random.randn(df_size, 4), columns=['A', 'B', 'C', 'D']).astype(np.float32)\n",
    "                pandopt_df = pdo.DataFramepandas_df)\n",
    "                pandas_checksum, pandas_time, pandas_error = measure_performance(pandas_df, func)\n",
    "                pandopt_checksum, pandopt_time, pandopt_error = measure_performance(pandopt_df, func)\n",
    "\n",
    "                key = f\"Size: 10^{test_num}, Func: {func.__name__}, Test: {test_type} - {test_iter}\"\n",
    "                results[key] = {\n",
    "                    \"Size\":df_size,\n",
    "                    \"function\": func.__name__, \n",
    "                    \"test_type\": test_type, \n",
    "                    \"test_iter\": test_iter,\n",
    "                    \"Pandas Time (s)\": pandas_time,\n",
    "                    \"Pandopt Time (s)\": pandopt_time,\n",
    "                    \"Checksum Pandas\": pandas_checksum,\n",
    "                    \"Checksum Pandopt\": pandopt_checksum,\n",
    "                    \"Pandas Error\": pandas_error,\n",
    "                    \"Pandopt Error\": pandopt_error\n",
    "                }\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "agg_functions = [agg_sum, agg_mean, agg_max, agg_min, agg_std]\n",
    "results_df = run_tests(data_size=8, agg_funcs=agg_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bbcce-f201-47e8-8701-7ad7c8c1a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['time reduction'] = results_df[\"Pandopt Time (s)\"] / results_df[\"Pandas Time (s)\"] - 1\n",
    "results_df['performance multiplicator'] = results_df[\"Pandas Time (s)\"] / results_df[\"Pandopt Time (s)\"] - 1\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3df65-ddb5-4e78-87e4-268694748e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('benchmark_rolling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec21d7e-f19a-4ed7-a415-dce7f5ae1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94abd32-bfa7-4982-abaa-2b68318cb2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb73e3-127c-4e90-8447-e21677c8782b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb83f6e-3828-4f14-99fe-be65c4d907c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
